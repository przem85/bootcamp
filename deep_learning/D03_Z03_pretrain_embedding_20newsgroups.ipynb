{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Dropout, Embedding, SimpleRNN, LSTM, Bidirectional\n",
    "from keras.callbacks import History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "* **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "* **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "# Zad. \n",
    "Podążamy za stroną: \n",
    "\n",
    "https://keras.io/examples/pretrained_word_embeddings/\n",
    "\n",
    "mamy jakiś zbiór tekstów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='train',categories=['sci.crypt', 'sci.electronics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: tedwards@eng.umd.edu (Thomas Grant Edwards)\n",
      "Subject: Re: Once tapped, your code is no good any more.\n",
      "Organization: Project GLUE, University of Maryland, College Park\n",
      "Lines: 25\n",
      "Distribution: na\n",
      "NNTP-Posting-Host: pipa.src.umd.edu\n",
      "\n",
      "In article <1r1r3nINNebn@dns1.NMSU.Edu> amolitor@nmsu.edu (Andrew Molitor) writes:\n",
      ">In article <C5so84.Hxv@demon.co.uk> Graham Toal <gtoal@gtoal.com> writes:\n",
      ">>Actually, I am *completely* baffled by why Dorothy Denning has chosen\n",
      ">>to throw away her academic respectability like this.\n",
      "\n",
      ">\tActually, I've been following her remarks for some time, with\n",
      ">interest. I'm also a member of academia, and her remarks have nothing\n",
      ">but elevate her respectability in my eyes. It remains to be seen whether\n",
      ">you are the radical fringe, or I.\n",
      "\n",
      ">\tIt is generally an error to assume that your beliefs are held by\n",
      ">the majority, or even a sizable minority. Especially when you're seeing\n",
      ">tens, nay dozens, of people on usenet agreeing with you.\n",
      "\n",
      "The people on usenet are clearly a special bunch.  We live the net, which\n",
      "is the future of our culture.  Usenetters have rapid electronic access to\n",
      "information.  Society in general must depend on CNN.  \n",
      "\n",
      "I can only hope we can make this information accessable by the public before\n",
      "the radical fringe, which _is_ the majority, destroys the fabric of\n",
      "this country.  Freedom is never easily won.\n",
      "\n",
      "-Thomas\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# newsgroups_train.data[0]\n",
    "print(newsgroups.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(y)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(mail):    \n",
    "    # wyciagniecie tresci maila   \n",
    "    return mail[mail.find(\"\\n\\n\"):]\n",
    "\n",
    "def extract_subject(mail):\n",
    "    return re.findall(r'Subject:(.+)',mail)[0]\n",
    "\n",
    "\n",
    "def stem_helper(word,stemmer):\n",
    "    try:\n",
    "        y = stemmer.stem(word)\n",
    "    except:\n",
    "        y = word\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(x, stemmer = nltk.PorterStemmer()):   \n",
    "    \"\"\"\n",
    "    x - jeden mail\n",
    "    \"\"\"    \n",
    " \n",
    "    #tokenizacja - rozbicie na liste tokenow\n",
    "    x_t = nltk.word_tokenize(x)    \n",
    "    # usuwanie znakow interpunkcyjnych\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    x_t = [word.translate(translator) for word in x_t]    \n",
    "    # zamina liter male\n",
    "    x_t = [word.lower() for word in x_t]    \n",
    "    # usuwanie zbednych tokenow\n",
    "    x_t = [w for w in x_t if w not in nltk.corpus.stopwords.words(\"english\")+[\"nt\"]]    \n",
    "    # stemming\n",
    "    x_t = [stem_helper(word,stemmer) for word in x_t]    \n",
    "    # sklejenie do napisu\n",
    "    x_new = ' '.join(x_t)    \n",
    "    return x_new\n",
    "\n",
    "\n",
    "X_c = [clean_text(extract_content(x)) for x in X]\n",
    "X_s = [clean_text(extract_subject(x)) for x in X]\n",
    "\n",
    "X = pd.DataFrame({\"subject\":X_s, \"content\":X_c})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_c,y,test_size=400)\n",
    "#bierzemy tylko tresc - bez tytulu\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'archivenam  ripemattack lastupd  31 mar 93 210000 0500 possibl attack ripem                live list potenti weak keep eye open use ripem secur electron mail  go great detail  almost certainli exhaust  obvious  mani weak weak cryptograph secur mail gener  pertain secur mail program ripem  maintain marc vanheyningen  mvanheyn  csindianaedu   post monthli varieti news group  followup pertain specif ripem go altsecurityripem  cryptanalysi attack             break rsa would allow attack find privat key  case could read mail encrypt sign messag privat key  rsa gener believ resist standard cryptanalyt techniqu  even standard key  516 bit ripem  long enough render impract  bar huge invest hardwar breakthrough factor   break de would allow attack read given messag  sinc messag encrypt de  would allow attack claim  de 56 bit key  thu could conceiv compromis brute forc suffici hardwar  agenc money devot simpli read one messag  sinc messag differ de key  work messag would remain high  key manag attack             steal privat key would allow benefit break rsa  safeguard  encrypt de key deriv passphras type  howev  attack get copi privat keyfil passphras  snoop network packet  tap line  whatev  could break whole scheme  main risk transfer either passphras privat key file across untrust link   run ripem trust machin  prefer one sit right front  ideal  machin home  mayb offic  nobodi els physic access   fool accept bogu public key someon els could allow oppon deceiv send secret messag rather real recipi  enemi fool intend recipi well  could reencrypt messag bogu public key pass along  import get proper public key peopl  common mechan finger  assum oppon compromis router daemon  finger given fair amount trust  strongest method key authent exchang key person  howev  alway practic  peopl  vouch  sign statement contain key possibl  although ripem featur automat pgp  ripem gener check md5 fingerprint public key key file  may exchang via separ channel authent  playback attack          even oppon break cryptographi  oppon could still caus difficulti  exampl  suppos send messag miconli  pem mode provid disclosur protect  alic say  ok  let   oppon intercept  resend bob  messag authent tell  cours  may interpret entir differ context  oppon could transmit messag recipi much later  figur would seen differ later time  oppon could chang originatornam   regist public key  send messag hope recipi send return mail indic  perhap even quot   unknown messag  defeat playback attack  plaintext messag includ indic sender recipi  uniqu identifi  typic date   good frontend script ripem automat  imho   recipi  sure originatornam  header sender indic within plaintext  realli recipi  messag old one  also autom  author faq made modest attempt autom process gener check encapsul header  program includ standard distribut util directori  local attack         clearli  secur ripem greater secur machin encrypt perform  exampl  unix  superus could manag get encrypt mail  although would take plan effort someth like replac ripem execut trojan hors get copi plaintext  depend store  addit  link machin run ripem extens  decrypt ripem remot machin connect via network   wors yet  modem   eavesdropp could see plaintext  probabl also passphras   ripem execut system trust  obvious  extrem case  ripem use machin  total control nobodi els access  care examin softwar known free virus   howev  real tradeoff conveni secur  moder cautiou user might use ripem unix workstat peopl access  even root access   increas secur keep privat key  static link  cours  execut floppi disk  peopl keep ripem multius system  dial insecur line  download messag system perform ripem decrypt  howev  secur provid mechan somewhat illusori  sinc presum type cleartext password log  given away store  sinc attack log instal trap account steal privat key next time use less insecur line  like remain situat long system use rather quaint mechan cleartext password authent  find nice put brief statement care manag secur arrang plan next public key  potenti correspond awar level precaut place  peopl use two key  short one care manag ordinari use longer one treat greater care critic correspond  untrust partner attack               ripem encrypt ensur person privat key correspond public key use encrypt data may read traffic  howev  someon key get messag  may alway make whatev kind transform wish  exist cryptograph barrier recipi  say  take encrypt messag convert miconli messag  sign readabl anyon  although ripem provid function  inde  latest pem draft seen specif state transform possibl allow forward function work  includ recipi plaintext  mention  make possibl recipi redistribut messag awar origin natur  natur  secur cryptographi never greater secur peopl use '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do warstwy **Embedding layer** wchodzi sekwencja intów.\n",
    "\n",
    "* my wykorzystamy reprezenatację Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekwencje mają różne długości, a Keras wymaga aby wejścia były równej długość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embeding ma zakres 100 i długość wejściową 20. Zmniejszmy embending do wymiaru 20.\n",
    "* Model jest prostym klasyfikatorem binarnym. \n",
    "* Co ważne, wynik z warstwy Embeding będzie wynosił 4 wektory o 8 wymiarach każdy, po jednym dla każdego słowa. \n",
    "* Spłaszczamy to do jednego 32-elementowego wektora, aby przejść do warstwy wyjściowej Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['accuracy'], label = \"tarina Adam\")\n",
    "plt.plot(history_1.history['val_accuracy'], label = \"test Adam\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain embedding\n",
    "\n",
    "https://keras.io/examples/pretrained_word_embeddings/\n",
    "\n",
    "* GloVe embedding data can be found at: http://nlp.stanford.edu/data/glove.6B.zip (source page: http://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "* After downloading and unzipping, you will see a few files, one of which is “glove.6B.50d.txt“, which contains a 100-dimensional version of the embedding.\n",
    "\n",
    "\n",
    "Pojedyńczy plik można pobrać z tąd:\n",
    "https://www.dropbox.com/sh/tjq47ybybgnrbel/AAAVbp0UkQTAbKWVMIi5mtHpa?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B.50d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a Tokenizer class that can be fit on the training data, can convert text to sequences consistently by calling the texts_to_sequences() method on the Tokenizer class, and provides access to the dictionary mapping of words to integers in a word_index attribute.\n",
    "\n",
    "https://keras.io/preprocessing/text/#tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekwencje mają różne długości, a Keras wymaga aby wejścia były równej długość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding.\n",
    "\n",
    "The result is a matrix of weights only for words we will see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference is that the embedding layer can be seeded with the GloVe word embedding weights. We chose the 50-dimensional version, therefore the Embedding layer must be defined with output_dim set to 50. Finally, we do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy: %f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['accuracy'], label = \"tarina Trainable\")\n",
    "plt.plot(history_1.history['val_accuracy'], label = \"test Trainable\")\n",
    "\n",
    "plt.plot(history_2.history['accuracy'], label = \"tarina Not Trainable\")\n",
    "plt.plot(history_2.history['val_accuracy'], label = \"test Not Trainable\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
