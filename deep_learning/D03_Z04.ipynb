{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Dropout, Embedding, SimpleRNN, LSTM, Bidirectional\n",
    "from keras.callbacks import History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "* **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "* **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "# Zad.\n",
    "\n",
    "Poberz dane z konkursu http://2017.poleval.pl/index.php/tasks/ Task 2. Sentiment analysis\n",
    "\n",
    "Pojedyńczy plik można pobrać z tąd:\n",
    "https://www.dropbox.com/sh/tjq47ybybgnrbel/AAAVbp0UkQTAbKWVMIi5mtHpa?dl=0\n",
    "\n",
    "## Zad\n",
    "Zbuduj model na podstawie warstwy Embedding, która się uczy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n"
     ]
    }
   ],
   "source": [
    "file_with_filtered_embeddings = \"Dane/data_poleval/embeddings.txt\"\n",
    "\n",
    "words2ids = {}\n",
    "embeddings = []\n",
    "\n",
    "embeddings.append(np.zeros(300)) # rezerwujemy embeddingi na paddin i nieznane slowa\n",
    "embeddings.append(np.zeros(300))\n",
    "\n",
    "i = 0\n",
    "with open(file_with_filtered_embeddings,\"r\", encoding=\"utf8\")as f:\n",
    "    for line in f:\n",
    "        toks = line.split(\" \")\n",
    "        word = toks[0]\n",
    "        embeddings.append(np.array([float(x) for x in toks[1:]]))\n",
    "        words2ids[word] = i+2 # +3 - przesuniecie po to zeby specjalne embeddingi byly na pozycji 0 i 1\n",
    "        i = i + 1\n",
    "\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2731"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2ids[\"Słodkawy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0695675 , -0.0123177 ,  0.0190564 ,  0.0205853 , -0.0586069 ,\n",
       "        0.123962  , -0.0113998 ,  0.0486168 , -0.0593128 ,  0.0244056 ,\n",
       "        0.00806909, -0.0123139 ,  0.0318254 ,  0.0375928 ,  0.0488764 ,\n",
       "       -0.0311151 , -0.0571845 ,  0.00886492, -0.023981  , -0.0729585 ,\n",
       "        0.00780728, -0.0103228 ,  0.0451543 , -0.00375595, -0.01016   ,\n",
       "        0.0157066 ,  0.0933215 ,  0.00742586,  0.0662036 ,  0.0147866 ,\n",
       "       -0.0326982 , -0.105277  ,  0.0939024 , -0.141137  , -0.0390246 ,\n",
       "        0.0307554 , -0.124878  ,  0.092467  , -0.0449775 ,  0.0278756 ,\n",
       "        0.0120974 ,  0.0402319 , -0.0602183 ,  0.117348  , -0.0286395 ,\n",
       "        0.0226578 , -0.0100841 ,  0.0928551 ,  0.0676152 , -0.0280563 ,\n",
       "        0.0516412 ,  0.0899037 , -0.0634045 ,  0.0149937 , -0.0472655 ,\n",
       "       -0.0638853 , -0.00977132, -0.0440674 ,  0.00945409, -0.0873957 ,\n",
       "        0.0113474 ,  0.00536351,  0.0406797 ,  0.051127  ,  0.0194782 ,\n",
       "        0.00793247,  0.00938793,  0.0813627 , -0.0323478 ,  0.00834836,\n",
       "       -0.0307443 , -0.00129854,  0.0432319 ,  0.133133  ,  0.025146  ,\n",
       "       -0.00617633, -0.0998303 ,  0.0437879 ,  0.139863  ,  0.0770133 ,\n",
       "        0.00529203, -0.0904055 ,  0.0356017 , -0.092935  ,  0.00502568,\n",
       "        0.00720983,  0.0144587 ,  0.0504662 ,  0.106105  , -0.0596305 ,\n",
       "        0.0165428 , -0.0756499 , -0.0548418 , -0.0436699 , -0.00525561,\n",
       "        0.0554075 ,  0.0690609 ,  0.09896   ,  0.0120319 , -0.0420232 ,\n",
       "        0.00035797, -0.0137097 ,  0.0175877 , -0.148398  , -0.0668958 ,\n",
       "        0.0681549 , -0.0670223 , -0.00222621, -0.047205  , -0.0354883 ,\n",
       "        0.0521945 , -0.00938699, -0.110972  , -0.0411835 , -0.0262399 ,\n",
       "       -0.0245242 , -0.0205056 ,  0.119049  , -0.0968168 ,  0.00101308,\n",
       "        0.0438422 ,  0.0132003 , -0.0552679 ,  0.0358181 ,  0.0659297 ,\n",
       "       -0.0691131 ,  0.0380908 ,  0.0678329 , -0.0180567 , -0.0153763 ,\n",
       "       -0.0928159 ,  0.0291021 ,  0.0661943 , -0.0232954 , -0.0999478 ,\n",
       "       -0.025268  ,  0.0265694 , -0.0789217 , -0.139136  , -0.043029  ,\n",
       "       -0.004668  ,  0.142589  ,  0.0491691 ,  0.0301554 , -0.0725976 ,\n",
       "       -0.0245116 , -0.0258727 , -0.081907  , -0.033818  , -0.0910337 ,\n",
       "        0.134159  , -0.0114324 ,  0.168495  ,  0.0744587 , -0.0551564 ,\n",
       "       -0.0299129 , -0.0764537 , -0.0583725 ,  0.101149  ,  0.0483574 ,\n",
       "        0.0185893 ,  0.0476507 , -0.00851304,  0.0257832 , -0.0423777 ,\n",
       "       -0.026234  , -0.0224656 , -0.0177372 , -0.0269845 , -0.0395937 ,\n",
       "       -0.0895522 , -0.0387699 ,  0.0386046 , -0.0269241 , -0.0252017 ,\n",
       "        0.00555369, -0.125734  ,  0.052703  , -0.0508996 , -0.0301596 ,\n",
       "        0.0719947 , -0.00781578,  0.0183722 ,  0.0137835 ,  0.145643  ,\n",
       "       -0.163511  ,  0.00493691,  0.00842864, -0.00523769,  0.0250127 ,\n",
       "       -0.069887  ,  0.0340264 , -0.0277308 ,  0.0666341 ,  0.0452207 ,\n",
       "        0.00394535,  0.159992  , -0.0478898 , -0.0356679 , -0.0167176 ,\n",
       "        0.0389694 ,  0.0144782 , -0.0481584 , -0.0269981 ,  0.0665881 ,\n",
       "       -0.00597176,  0.0203096 , -0.0116472 , -0.0288414 , -0.0123771 ,\n",
       "       -0.0126907 ,  0.0264299 , -0.094251  ,  0.0308239 , -0.0451183 ,\n",
       "       -0.00945021, -0.073934  ,  0.0713121 , -0.0174938 ,  0.0587443 ,\n",
       "       -0.00283306, -0.0534264 ,  0.0672013 , -0.0402565 , -0.0248949 ,\n",
       "       -0.0521673 , -0.00848431,  0.0723062 , -0.00412129,  0.0725852 ,\n",
       "        0.068625  ,  0.00367457, -0.0591261 , -0.113534  ,  0.0148794 ,\n",
       "       -0.0679527 ,  0.0171171 , -0.0326562 ,  0.087949  , -0.0723946 ,\n",
       "       -0.0454645 , -0.0426597 ,  0.0783788 ,  0.0905034 ,  0.05728   ,\n",
       "       -0.0858484 , -0.064867  ,  0.0215164 ,  0.0423614 , -0.0340275 ,\n",
       "        0.0465895 ,  0.0276519 ,  0.0410746 ,  0.0158002 , -0.154203  ,\n",
       "        0.0731088 ,  0.0101519 , -0.0274256 , -0.0583854 , -0.0184802 ,\n",
       "       -0.0752445 ,  0.0297846 ,  0.0426677 , -0.0593996 ,  0.0168314 ,\n",
       "        0.0689239 , -0.00937583, -0.00648955, -0.0126788 ,  0.0210303 ,\n",
       "        0.0957282 , -0.0613672 , -0.0333282 , -0.0210865 , -0.0843227 ,\n",
       "        0.0578791 ,  0.052726  , -0.0566956 , -0.0407154 ,  0.0603111 ,\n",
       "       -0.0569204 , -0.0586587 , -0.0135658 ,  0.0254326 , -0.0944752 ,\n",
       "       -0.07885   , -0.00470421,  0.0174975 ,  0.0462485 ,  0.0642261 ,\n",
       "        0.0310251 ,  0.102567  , -0.041987  ,  0.0700113 , -0.0171799 ,\n",
       "        0.0922816 ,  0.00777218, -0.0109217 , -0.090305  , -0.0368232 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[words2ids[\"Słodkawy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence as seq\n",
    "def load_and_transform_data_to_phrases(labels, parents, tokens, words2ids):\n",
    "\n",
    "    \"\"\"\n",
    "    Dokumentacja\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    transform_label = {'-1':0, '0':1, '1':2}\n",
    "    \n",
    "    l = open(labels, \"r\", encoding=\"utf8\")\n",
    "    labels = [[transform_label[y] for y in x.split()] for x in l.readlines()] \n",
    "    l.close()\n",
    "\n",
    "    p = open(parents,\"r\", encoding=\"utf8\")\n",
    "    parents = [[int(y) for y in x.split()] for x in p.readlines()]\n",
    "    p.close()\n",
    "\n",
    "    t = open(tokens,\"r\", encoding=\"utf8\")\n",
    "    tokens = [x.split() for x in t.readlines()]\n",
    "    t.close()\n",
    "    \n",
    "    k = 0\n",
    "    result = []\n",
    "    \n",
    "    for labels_i,parents_i,tokens_i in zip(labels,parents,tokens):\n",
    "        \n",
    "        k = k + 1\n",
    "         \n",
    "        s = []\n",
    "        for i in range(len(tokens_i)):\n",
    "            s.append([i,int(parents_i[i]),labels_i[i],tokens_i[i]])\n",
    "\n",
    "\n",
    "        if len(s) == 1: #przypadek gdy fraza sklada sie z jednego tokena\n",
    "\n",
    "            result.append((\\\n",
    "                                  tokens[0],\n",
    "                                  np.array([words2ids.get(tokens[0], 1)]),\\\n",
    "                                  np.array(labels_i[0]) \\\n",
    "                              ))    \n",
    "                           \n",
    "        else: \n",
    "            \n",
    "            for i in range(len(s)): \n",
    "                children = []\n",
    "                for j in range(len(s)):\n",
    "                    if s[j][1] == i+1:\n",
    "                        children.append(s[j][0])\n",
    "                s[i].append(children)\n",
    "\n",
    "                \n",
    "            words = [x[0] for x in s]\n",
    "            children = [x[4] for x in s]\n",
    "            tokens = [x[3] for x in s]\n",
    "            labels_in_batch = [x[2] for x in s]\n",
    "        \n",
    "            phrases = [[k] for k in range(len(children))]\n",
    "            for i in range(len(children)):\n",
    "                for e in phrases[i]:\n",
    "                    phrases[i].extend(children[e])\n",
    "           \n",
    "            phrases = [ np.sort(x) for x in phrases]\n",
    "          \n",
    "            phrases = list(zip([np.array(tokens_i)[x] for x in phrases],\n",
    "                               [np.array([words2ids.get(t,1) for t in tokens_i])[x] for x in phrases],\n",
    "                               labels_i))\n",
    "\n",
    "            result.extend(phrases)\n",
    "           \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_and_transform_data_to_phrases(\"Dane/data_poleval/training-treebank/rev_labels.txt\", \"Dane/data_poleval/training-treebank/rev_parents.txt\",\"Dane/data_poleval/training-treebank/rev_sentence.txt\",words2ids)\n",
    "test_data = load_and_transform_data_to_phrases(\"Dane/data_poleval/poleval_test/gold_labels\", \"Dane/data_poleval/poleval_test/polevaltest_parents.txt\",\"Dane/data_poleval/poleval_test/polevaltest_sentence.txt\",words2ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy: %f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['categorical_accuracy'], label = \"tarina\")\n",
    "plt.plot(history_1.history['val_categorical_accuracy'], label = \"test\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Użyj Pretrain embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: %f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['categorical_accuracy'], label = \"tarina Trainable\")\n",
    "plt.plot(history_1.history['val_categorical_accuracy'], label = \"test Trainable\")\n",
    "\n",
    "plt.plot(history_2.history['categorical_accuracy'], label = \"tarina Not Trainable\")\n",
    "plt.plot(history_2.history['val_categorical_accuracy'], label = \"test Not Trainable\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Użyj architektury\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 300,\n",
    "                   input_length=max_len,\n",
    "                  weights=[embeddings],\n",
    "                  trainable=False))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Użyj architektury\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 300,\n",
    "                   input_length=max_len,\n",
    "                  weights=[embeddings],\n",
    "                  trainable=False))\n",
    "model.add(Conv1D(50,3))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "Co dokładnie robi \n",
    "\n",
    "```python\n",
    "from keras.layers.convolutional import Conv1D\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Użyj architektury\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 300,\n",
    "                   input_length=max_len,\n",
    "                  weights=[embeddings],\n",
    "                  trainable=False))\n",
    "model.add(TimeDistributed(Dense(10,activation=\"tanh\")))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "Co dokładnie robi \n",
    "\n",
    "```python\n",
    "from keras.layers import TimeDistributed\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
