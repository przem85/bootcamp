{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library necessary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re, string, unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "Pobierz dane ze strony:\n",
    "https://www.kaggle.com/c/sa-emotions/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), 'data', 'train_data.csv')) \n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.join(os.getcwd(), 'data', 'train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "content      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:1000] \n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Zobacz ile jest klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x3316fcb148>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_freq = df['sentiment'].value_counts()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sentiment_freq.plot(ax=ax, kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Podzielmy atrybuty na część \n",
    "* objaśniającą $X$\n",
    "* objaśnianą $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['sentiment'], axis=1)\n",
    "y = df['sentiment'].values\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "Przyjrzyjmy się $y$. Musi to być kolumna numeryczna z labealmi.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (1000, 1) y.shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"X.shape: {} y.shape: {}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, alpha=0.5)\n",
    "plt.hist(y_test, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Zbudujmy **pipeline** dla atrybutu \"content\":\n",
    "\n",
    " * usuń adresy stron interntowych\n",
    " * usuń słowa zaczynające się od \"@\"\n",
    " * usuń punktory\n",
    " * usuń liczby (lub zamień je na jeden token \"<NUMBER\\>\")\n",
    " * zamień wszystkie słowa na zaczynające się z małej litery\n",
    " * wygeneruj reprezentację    \n",
    " * Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# A class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library necessary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "\n",
    "# import enchant\n",
    "# eng_dict = enchant.Dict(\"en_US\")\n",
    "words_to_save = ['<EOS>', '<BOS>']\n",
    "\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "def remove_urls(text):\n",
    "    txt = re.sub(r'pic\\.twitter\\.com.*?( |$)', '', text)\n",
    "    txt = re.sub(r'http.+?\\xa0', '', txt)\n",
    "    return re.sub(r'http.+?( |$)', '', txt)\n",
    "\n",
    "def remove_marks(text):\n",
    "    return re.sub(r'@ ?.*?( |$)', '', text)\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def sen2token(sentence):\n",
    "    sentence = remove_urls(sentence)\n",
    "    sentence = remove_marks(sentence)\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in words_to_save:\n",
    "            new_words.append(word)\n",
    "            continue\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def add_tags(words):\n",
    "    new_words = []\n",
    "    for word_id, word in enumerate(words):\n",
    "        if word in ['...', '.', '!', '?']:\n",
    "            new_words.append('<EOS>')\n",
    "            new_words.append('<BOS>')\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    new_words.insert(0, '<BOS>')\n",
    "    if new_words[-1] == '<BOS>':\n",
    "        return new_words[:-1]\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_lower(words):\n",
    "    return [w.lower() for w in words]\n",
    "\n",
    "def remove_nummbers(words):\n",
    "    return [w for w in words if not w.isdigit()]\n",
    "\n",
    "def stemming_tokenizer(words):\n",
    "    words = sen2token(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lower(words)     \n",
    "#     words = remove_punctuation(words)\n",
    "#     words = remove_no_english(words)    \n",
    "    words = remove_nummbers(words)\n",
    "\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "#preprocessor działa na całym dokumencie\n",
    "def my_preprocessing(word):\n",
    "    return word\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor = my_preprocessing, \n",
    "                tokenizer=stemming_tokenizer, \n",
    "                stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# tfidf_vectorizer.fit(X)\n",
    "# # print( tfidf_vectorizer.vocabulary_ )\n",
    "# tfidf_matrix = tfidf_vectorizer.transform(X)\n",
    "# # print(tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 2203)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "\n",
    "class ToListEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.values.T.tolist()[0]\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "        (\"select_cat\", DataFrameSelector(['content'])),\n",
    "        (\"to_numpy\", ToListEncoder()),\n",
    "        (\"dictionary_encoder\", tfidf_vectorizer),\n",
    "    ])\n",
    "\n",
    "X_tr = preprocess_pipeline.fit_transform(X_train)\n",
    "X_tr\n",
    "X_tr.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'know',\n",
       " 'i',\n",
       " 'was',\n",
       " 'listenin',\n",
       " 'to',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'earlier',\n",
       " 'and',\n",
       " 'i',\n",
       " 'started',\n",
       " 'freakin',\n",
       " 'at',\n",
       " 'his',\n",
       " 'part',\n",
       " '=',\n",
       " '[']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen2token()\n",
    "print(df.content.values[0])\n",
    "sen2token(df.content.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hmmm. http://www.djhero.com/ is down\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hmmm', '.', 'is', 'down']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen2token()\n",
    "print(df.content.values[7])\n",
    "sen2token(df.content.values[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Robimy StratifiedKFold i znajdujemy optymalne parametry dla\n",
    "\n",
    "\n",
    "* MultinomialNB (bez redukcji wymiarowości)\n",
    "* LogisticRegression\n",
    "* LinearSVC\n",
    "* SVC\n",
    "* KNeighborsClassifier\n",
    "* DecisionTreeClassifier\n",
    "* RandomForestClassifier\n",
    "* BaggingClassifier\n",
    "* ExtraTreesClassifier\n",
    "* AdaBoostClassifier\n",
    "* GradientBoostingClassifier\n",
    "* VotingClassifier\n",
    "* xgboost.XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed=123\n",
    "kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:668: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline), \n",
    "    ('classifier', LinearSVC())])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_1 = GridSearchCV(pipe, param_grid, cv=kfold)\n",
    "\n",
    "grid_1.fit(X_train, y_train)\n",
    "grid_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM linear\n",
      "precision_score: 0.2554543650793651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_score: 0.335\n",
      "f1_score: 0.2890102874043593\n",
      "accuracy_score: 0.335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import  metrics\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('SVM linear', grid_1.best_estimator_))\n",
    "\n",
    "\n",
    "\n",
    "precision_score = []\n",
    "recall_score = []\n",
    "f1_score = []\n",
    "accuracy_score = []\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    print(\"precision_score: {}\".format(metrics.precision_score(y_test, model.predict(X_test), average='weighted') ))\n",
    "    print(\"recall_score: {}\".format( metrics.recall_score(y_test, model.predict(X_test), average='weighted') ))\n",
    "    print(\"f1_score: {}\".format( metrics.f1_score(y_test, model.predict(X_test), average='weighted') ))\n",
    "    print(\"accuracy_score: {}\".format( metrics.accuracy_score(y_test, model.predict(X_test)) ))\n",
    "    precision_score.append(metrics.precision_score(y_test, model.predict(X_test), average='weighted'))\n",
    "    recall_score.append(metrics.recall_score(y_test, model.predict(X_test), average='weighted'))\n",
    "    f1_score.append( metrics.f1_score(y_test, model.predict(X_test), average='weighted'))\n",
    "    accuracy_score.append(metrics.accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM linear</td>\n",
       "      <td>0.255454</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.28901</td>\n",
       "      <td>0.335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Method  precision_score  recall_score  f1_score  accuracy_score\n",
       "0  SVM linear         0.255454         0.335   0.28901           0.335"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {'precision_score': precision_score, \n",
    "     'recall_score': recall_score, \n",
    "     'f1_score': f1_score,\n",
    "     'accuracy_score' : accuracy_score\n",
    "    }\n",
    "df = pd.DataFrame(data=d)\n",
    "df.insert(loc=0, column='Method', value=['SVM linear'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
