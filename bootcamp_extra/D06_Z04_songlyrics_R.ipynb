{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library necessary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad. \n",
    "Wczytaj dane zawierające informacje o piosenkach.\n",
    "\n",
    "Dane trzeba rozpakować.\n",
    "\n",
    "https://www.kaggle.com/laowingkin/song-text-mining-and-clustering/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  And i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  Touch me gently ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  Why I had to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Burning My Bridges</td>\n",
       "      <td>/a/abba/burning+my+bridges_20003011.html</td>\n",
       "      <td>Well, you hoot and you holler and you make me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Cassandra</td>\n",
       "      <td>/a/abba/cassandra_20002811.html</td>\n",
       "      <td>Down in the street they're all singing and sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Chiquitita</td>\n",
       "      <td>/a/abba/chiquitita_20002978.html</td>\n",
       "      <td>Chiquitita, tell me what's wrong  You're encha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Crazy World</td>\n",
       "      <td>/a/abba/crazy+world_20003013.html</td>\n",
       "      <td>I was out with the morning sun  Couldn't sleep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Crying Over You</td>\n",
       "      <td>/a/abba/crying+over+you_20177611.html</td>\n",
       "      <td>I'm waitin' for you baby  I'm sitting all alon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "5   ABBA     Burning My Bridges    /a/abba/burning+my+bridges_20003011.html   \n",
       "6   ABBA              Cassandra             /a/abba/cassandra_20002811.html   \n",
       "7   ABBA             Chiquitita            /a/abba/chiquitita_20002978.html   \n",
       "8   ABBA            Crazy World           /a/abba/crazy+world_20003013.html   \n",
       "9   ABBA        Crying Over You       /a/abba/crying+over+you_20177611.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  And i...  \n",
       "1  Take it easy with me, please  Touch me gently ...  \n",
       "2  I'll never know why I had to go  Why I had to ...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  \n",
       "5  Well, you hoot and you holler and you make me ...  \n",
       "6  Down in the street they're all singing and sho...  \n",
       "7  Chiquitita, tell me what's wrong  You're encha...  \n",
       "8  I was out with the morning sun  Couldn't sleep...  \n",
       "9  I'm waitin' for you baby  I'm sitting all alon...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('songdata.csv')\n",
    "df['text'] = df['text'].str.replace('\\n', '')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zmniejszmy troszkę zbiór danych\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist    0\n",
       "song      0\n",
       "link      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:1000] \n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# y = df[\"artist\"].values[:1000]\n",
    "# X = df[\"text\"].values[:1000]\n",
    "\n",
    "X = df.drop(['artist'], axis=1)\n",
    "y = df['artist'].values\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  9  9 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13\n",
      " 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n",
      " 13 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
      " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 18 18 18 18\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n",
      " 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 21 21 21 21 22 22 22 22 22\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
      " 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
      " 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
      " 24 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25\n",
      " 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "# print(y)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# print(\"X.shape: {} y.shape: {}\".format(X.shape, y.shape))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, alpha=0.5)\n",
    "plt.hist(y_test, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# A class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library necessary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "\n",
    "def sen2token(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    remove_list =  stopwords.words('english') + list(string.punctuation) + ['--', '.\"', '!\"', '?\"', ',\"', '``', \"''\"]\n",
    "    return [w for w in words if not w in set(remove_list)]\n",
    "\n",
    "def remove_nummbers(words):\n",
    "    return [w for w in words if not w.isdigit()]\n",
    "\n",
    "def to_lower(words):\n",
    "    return [w.lower() for w in words]\n",
    "\n",
    "def stemming_tokenizer(words):\n",
    "    words = sen2token(words)\n",
    "    words = to_lower(words)    \n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_nummbers(words)\n",
    "\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "#preprocessor działa na całym dokumencie\n",
    "def my_preprocessing(word):\n",
    "    return word\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor = my_preprocessing, \n",
    "                tokenizer=stemming_tokenizer, \n",
    "                stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# tfidf_vectorizer.fit(X)\n",
    "# # print( tfidf_vectorizer.vocabulary_ )\n",
    "# tfidf_matrix = tfidf_vectorizer.transform(X)\n",
    "# # print(tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 865)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class ToListEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.values.T.tolist()[0]\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "        (\"select_cat\", DataFrameSelector(['song','text'])),\n",
    "        (\"to_numpy\", ToListEncoder()),\n",
    "        (\"dictionary_encoder\", tfidf_vectorizer),\n",
    "    ])\n",
    "\n",
    "X_tr = preprocess_pipeline.fit_transform(X_train)\n",
    "X_tr\n",
    "X_tr.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Robimy StratifiedKFold i znajdujemy optymalne parametry dla\n",
    "\n",
    "\n",
    "* MultinomialNB (bez redukcji wymiarowości)\n",
    "* LogisticRegression\n",
    "* LinearSVC\n",
    "* SVC\n",
    "* KNeighborsClassifier\n",
    "* DecisionTreeClassifier\n",
    "* RandomForestClassifier\n",
    "* BaggingClassifier\n",
    "* ExtraTreesClassifier\n",
    "* AdaBoostClassifier\n",
    "* GradientBoostingClassifier\n",
    "* VotingClassifier\n",
    "* xgboost.XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed=123\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:657: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline), \n",
    "    ('classifier', SVC(kernel='linear'))])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_1 = GridSearchCV(pipe, param_grid, cv=kfold)\n",
    "\n",
    "grid_1.fit(X_train, y_train)\n",
    "grid_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM linear\n",
      "precision_score: 0.1358245526995527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_score: 0.185\n",
      "f1_score: 0.1434947542628686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.185\n"
     ]
    }
   ],
   "source": [
    "from sklearn import  metrics\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('SVM linear', grid_1.best_estimator_))\n",
    "\n",
    "\n",
    "\n",
    "precision_score = []\n",
    "recall_score = []\n",
    "f1_score = []\n",
    "accuracy_score = []\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    print(\"precision_score: {}\".format(metrics.precision_score(y_test, model.predict(X_test), average='weighted') ))\n",
    "    print(\"recall_score: {}\".format( metrics.recall_score(y_test, model.predict(X_test), average='weighted') ))\n",
    "    print(\"f1_score: {}\".format( metrics.f1_score(y_test, model.predict(X_test), average='weighted') ))\n",
    "    print(\"accuracy_score: {}\".format( metrics.accuracy_score(y_test, model.predict(X_test)) ))\n",
    "    precision_score.append(metrics.precision_score(y_test, model.predict(X_test), average='weighted'))\n",
    "    recall_score.append(metrics.recall_score(y_test, model.predict(X_test), average='weighted'))\n",
    "    f1_score.append( metrics.f1_score(y_test, model.predict(X_test), average='weighted'))\n",
    "    accuracy_score.append(metrics.accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM linear</td>\n",
       "      <td>0.135825</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.143495</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Method  precision_score  recall_score  f1_score  accuracy_score\n",
       "0  SVM linear         0.135825         0.185  0.143495           0.185"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {'precision_score': precision_score, \n",
    "     'recall_score': recall_score, \n",
    "     'f1_score': f1_score,\n",
    "     'accuracy_score' : accuracy_score\n",
    "    }\n",
    "df = pd.DataFrame(data=d)\n",
    "df.insert(loc=0, column='Method', value=['SVM linear'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
