{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library necessary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad. \n",
    "Wczytaj dane zawierające informacje o piosenkach.\n",
    "\n",
    "Dane trzeba rozpakować.\n",
    "\n",
    "https://www.kaggle.com/laowingkin/song-text-mining-and-clustering/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  And i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  Touch me gently ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  Why I had to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Burning My Bridges</td>\n",
       "      <td>/a/abba/burning+my+bridges_20003011.html</td>\n",
       "      <td>Well, you hoot and you holler and you make me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Cassandra</td>\n",
       "      <td>/a/abba/cassandra_20002811.html</td>\n",
       "      <td>Down in the street they're all singing and sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Chiquitita</td>\n",
       "      <td>/a/abba/chiquitita_20002978.html</td>\n",
       "      <td>Chiquitita, tell me what's wrong  You're encha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Crazy World</td>\n",
       "      <td>/a/abba/crazy+world_20003013.html</td>\n",
       "      <td>I was out with the morning sun  Couldn't sleep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Crying Over You</td>\n",
       "      <td>/a/abba/crying+over+you_20177611.html</td>\n",
       "      <td>I'm waitin' for you baby  I'm sitting all alon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "5   ABBA     Burning My Bridges    /a/abba/burning+my+bridges_20003011.html   \n",
       "6   ABBA              Cassandra             /a/abba/cassandra_20002811.html   \n",
       "7   ABBA             Chiquitita            /a/abba/chiquitita_20002978.html   \n",
       "8   ABBA            Crazy World           /a/abba/crazy+world_20003013.html   \n",
       "9   ABBA        Crying Over You       /a/abba/crying+over+you_20177611.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  And i...  \n",
       "1  Take it easy with me, please  Touch me gently ...  \n",
       "2  I'll never know why I had to go  Why I had to ...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  \n",
       "5  Well, you hoot and you holler and you make me ...  \n",
       "6  Down in the street they're all singing and sho...  \n",
       "7  Chiquitita, tell me what's wrong  You're encha...  \n",
       "8  I was out with the morning sun  Couldn't sleep...  \n",
       "9  I'm waitin' for you baby  I'm sitting all alon...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('songdata.csv')\n",
    "df['text'] = df['text'].str.replace('\\n', '')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zmniejszmy troszkę zbiór danych\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist    0\n",
       "song      0\n",
       "link      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:1000] \n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# y = df[\"artist\"].values[:1000]\n",
    "# X = df[\"text\"].values[:1000]\n",
    "\n",
    "X = df.drop(['artist'], axis=1)\n",
    "y = df['artist'].values\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ABBA', 'Ace Of Base', 'Adam Sandler', 'Adele', 'Aerosmith',\n",
       "       'Air Supply', 'Aiza Seguerra', 'Alabama', 'Alan Parsons Project',\n",
       "       'Aled Jones', 'Alice Cooper', 'Alice In Chains', 'Alison Krauss',\n",
       "       'Allman Brothers Band', 'Alphaville', 'America', 'Amy Grant',\n",
       "       'Andrea Bocelli', 'Andy Williams', 'Annie', 'Ariana Grande',\n",
       "       'Ariel Rivera', 'Arlo Guthrie', 'Arrogant Worms', 'Avril Lavigne',\n",
       "       'Backstreet Boys'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA' 'ABBA'\n",
      " 'ABBA' 'ABBA' 'ABBA' 'Ace Of Base' 'Ace Of Base' 'Ace Of Base'\n",
      " 'Ace Of Base' 'Ace Of Base' 'Ace Of Base' 'Ace Of Base' 'Ace Of Base'\n",
      " 'Ace Of Base' 'Ace Of Base' 'Ace Of Base' 'Ace Of Base' 'Adam Sandler'\n",
      " 'Adam Sandler' 'Adam Sandler' 'Adam Sandler' 'Adam Sandler'\n",
      " 'Adam Sandler' 'Adam Sandler' 'Adam Sandler' 'Adele' 'Adele' 'Adele'\n",
      " 'Adele' 'Adele' 'Adele' 'Adele' 'Adele' 'Adele' 'Adele' 'Adele' 'Adele'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith' 'Aerosmith'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply' 'Air Supply'\n",
      " 'Air Supply' 'Aiza Seguerra' 'Aiza Seguerra' 'Aiza Seguerra'\n",
      " 'Aiza Seguerra' 'Aiza Seguerra' 'Aiza Seguerra' 'Aiza Seguerra'\n",
      " 'Aiza Seguerra' 'Aiza Seguerra' 'Aiza Seguerra' 'Aiza Seguerra'\n",
      " 'Aiza Seguerra' 'Aiza Seguerra' 'Aiza Seguerra' 'Aiza Seguerra' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama' 'Alabama'\n",
      " 'Alabama' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Alan Parsons Project'\n",
      " 'Alan Parsons Project' 'Alan Parsons Project' 'Aled Jones' 'Aled Jones'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice Cooper'\n",
      " 'Alice Cooper' 'Alice Cooper' 'Alice Cooper' 'Alice In Chains'\n",
      " 'Alice In Chains' 'Alice In Chains' 'Alice In Chains' 'Alice In Chains'\n",
      " 'Alice In Chains' 'Alice In Chains' 'Alice In Chains' 'Alice In Chains'\n",
      " 'Alice In Chains' 'Alice In Chains' 'Alice In Chains' 'Alice In Chains'\n",
      " 'Alice In Chains' 'Alice In Chains' 'Alice In Chains' 'Alice In Chains'\n",
      " 'Alice In Chains' 'Alice In Chains' 'Alice In Chains' 'Alice In Chains'\n",
      " 'Alice In Chains' 'Alison Krauss' 'Alison Krauss' 'Alison Krauss'\n",
      " 'Alison Krauss' 'Alison Krauss' 'Alison Krauss' 'Alison Krauss'\n",
      " 'Alison Krauss' 'Alison Krauss' 'Alison Krauss' 'Alison Krauss'\n",
      " 'Alison Krauss' 'Alison Krauss' 'Alison Krauss' 'Alison Krauss'\n",
      " 'Alison Krauss' 'Alison Krauss' 'Alison Krauss' 'Alison Krauss'\n",
      " 'Alison Krauss' 'Alison Krauss' 'Alison Krauss' 'Alison Krauss'\n",
      " 'Alison Krauss' 'Alison Krauss' 'Alison Krauss' 'Alison Krauss'\n",
      " 'Alison Krauss' 'Alison Krauss' 'Alison Krauss' 'Alison Krauss'\n",
      " 'Alison Krauss' 'Alison Krauss' 'Alison Krauss' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Allman Brothers Band' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Allman Brothers Band' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Allman Brothers Band' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Allman Brothers Band' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Allman Brothers Band' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Allman Brothers Band' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Allman Brothers Band' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Allman Brothers Band' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Allman Brothers Band' 'Allman Brothers Band'\n",
      " 'Allman Brothers Band' 'Alphaville' 'Alphaville' 'Alphaville'\n",
      " 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville'\n",
      " 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville'\n",
      " 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville'\n",
      " 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville'\n",
      " 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville'\n",
      " 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville' 'Alphaville'\n",
      " 'Alphaville' 'Alphaville' 'America' 'America' 'America' 'America'\n",
      " 'America' 'America' 'America' 'America' 'America' 'America' 'America'\n",
      " 'America' 'America' 'America' 'America' 'America' 'America' 'America'\n",
      " 'America' 'America' 'America' 'America' 'America' 'America' 'America'\n",
      " 'America' 'America' 'America' 'America' 'America' 'America' 'America'\n",
      " 'America' 'America' 'America' 'America' 'America' 'America' 'America'\n",
      " 'America' 'America' 'America' 'America' 'America' 'America' 'America'\n",
      " 'America' 'America' 'America' 'America' 'America' 'America' 'America'\n",
      " 'America' 'America' 'America' 'America' 'America' 'America' 'America'\n",
      " 'America' 'America' 'America' 'America' 'America' 'Amy Grant' 'Amy Grant'\n",
      " 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant'\n",
      " 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant'\n",
      " 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant'\n",
      " 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant'\n",
      " 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant' 'Amy Grant'\n",
      " 'Amy Grant' 'Amy Grant' 'Andrea Bocelli' 'Andrea Bocelli'\n",
      " 'Andrea Bocelli' 'Andrea Bocelli' 'Andrea Bocelli' 'Andy Williams'\n",
      " 'Andy Williams' 'Andy Williams' 'Andy Williams' 'Andy Williams'\n",
      " 'Andy Williams' 'Andy Williams' 'Andy Williams' 'Andy Williams'\n",
      " 'Andy Williams' 'Andy Williams' 'Andy Williams' 'Andy Williams'\n",
      " 'Andy Williams' 'Andy Williams' 'Andy Williams' 'Andy Williams'\n",
      " 'Andy Williams' 'Andy Williams' 'Andy Williams' 'Andy Williams'\n",
      " 'Andy Williams' 'Andy Williams' 'Andy Williams' 'Andy Williams'\n",
      " 'Andy Williams' 'Andy Williams' 'Andy Williams' 'Annie' 'Annie' 'Annie'\n",
      " 'Annie' 'Annie' 'Ariana Grande' 'Ariana Grande' 'Ariana Grande'\n",
      " 'Ariana Grande' 'Ariana Grande' 'Ariana Grande' 'Ariana Grande'\n",
      " 'Ariana Grande' 'Ariana Grande' 'Ariana Grande' 'Ariel Rivera'\n",
      " 'Ariel Rivera' 'Ariel Rivera' 'Ariel Rivera' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie' 'Arlo Guthrie'\n",
      " 'Arlo Guthrie' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms' 'Arrogant Worms'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne' 'Avril Lavigne'\n",
      " 'Avril Lavigne' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys'\n",
      " 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys'\n",
      " 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys'\n",
      " 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys'\n",
      " 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys'\n",
      " 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys'\n",
      " 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys'\n",
      " 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys'\n",
      " 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys'\n",
      " 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys' 'Backstreet Boys']\n",
      "X.shape: (1000, 3) y.shape: (1000,)\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  9  9 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13\n",
      " 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n",
      " 13 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
      " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 18 18 18 18\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n",
      " 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 21 21 21 21 22 22 22 22 22\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 22 22 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
      " 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
      " 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
      " 24 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25\n",
      " 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"X.shape: {} y.shape: {}\".format(X.shape, y.shape))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, alpha=0.5)\n",
    "plt.hist(y_test, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# A class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library necessary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "\n",
    "def sen2token(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    remove_list =  stopwords.words('english') + list(string.punctuation) + ['--', '.\"', '!\"', '?\"', ',\"', '``', \"''\"]\n",
    "    return [w for w in words if not w in set(remove_list)]\n",
    "\n",
    "def remove_nummbers(words):\n",
    "    return [w for w in words if not w.isdigit()]\n",
    "\n",
    "def to_lower(words):\n",
    "    return [w.lower() for w in words]\n",
    "\n",
    "\n",
    "\n",
    "def stemming_tokenizer(words):\n",
    "    words = sen2token(words)\n",
    "    words = to_lower(words)    \n",
    "#     words = add_tags(words)\n",
    "    words = remove_nummbers(words)\n",
    "\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "\n",
    "\n",
    "#preprocessor działa na całym dokumencie\n",
    "def my_preprocessing(word):\n",
    "    return word\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor = my_preprocessing, \n",
    "                tokenizer=stemming_tokenizer, \n",
    "                stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# tfidf_vectorizer.fit(X)\n",
    "# # print( tfidf_vectorizer.vocabulary_ )\n",
    "# tfidf_matrix = tfidf_vectorizer.transform(X)\n",
    "# # print(tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 876)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ToListEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.values.T.tolist()[0]\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "        (\"select_cat\", DataFrameSelector(['song','text'])),\n",
    "        (\"to_numpy\", ToListEncoder()),\n",
    "        (\"dictionary_encoder\", tfidf_vectorizer),\n",
    "    ])\n",
    "\n",
    "X_tr = preprocess_pipeline.fit_transform(X_train)\n",
    "X_tr\n",
    "X_tr.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Robimy StratifiedKFold i znajdujemy optymalne parametry dla\n",
    "\n",
    "\n",
    "* MultinomialNB (bez redukcji wymiarowości)\n",
    "* LogisticRegression\n",
    "* LinearSVC\n",
    "* SVC\n",
    "* KNeighborsClassifier\n",
    "* DecisionTreeClassifier\n",
    "* RandomForestClassifier\n",
    "* BaggingClassifier\n",
    "* ExtraTreesClassifier\n",
    "* AdaBoostClassifier\n",
    "* GradientBoostingClassifier\n",
    "* VotingClassifier\n",
    "* xgboost.XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed=123\n",
    "kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline), \n",
    "    ('classifier', LinearSVC())])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_1 = GridSearchCV(pipe, param_grid, cv=kfold)\n",
    "\n",
    "grid_1.fit(X_train, y_train)\n",
    "grid_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline), \n",
    "    ('classifier', LinearSVC())])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_1 = GridSearchCV(pipe, param_grid, cv=kfold)\n",
    "\n",
    "grid_1.fit(X_train, y_train)\n",
    "grid_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__alpha': 0.1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline), \n",
    "    ('classifier', MultinomialNB())])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__alpha': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_2 = GridSearchCV(pipe, param_grid, cv=kfold)\n",
    "\n",
    "grid_2.fit(X_train, y_train)\n",
    "grid_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 100, 'classifier__gamma': 0.01}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline), \n",
    "    ('PCA', TruncatedSVD(n_components=100)),\n",
    "    ('classifier', SVC())])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100]    \n",
    "}\n",
    "\n",
    "grid_3 = GridSearchCV(pipe, param_grid, cv=kfold)\n",
    "\n",
    "grid_3.fit(X_train, y_train)\n",
    "grid_3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM linear\n",
      "precision_score: 0.16457428760597775\n",
      "recall_score: 0.18\n",
      "f1_score: 0.15245561444468403\n",
      "accuracy_score: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "precision_score: 0.13113542546205084\n",
      "recall_score: 0.175\n",
      "f1_score: 0.14039281775803514\n",
      "accuracy_score: 0.175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM EBF\n",
      "precision_score: 0.15079790876527602\n",
      "recall_score: 0.18\n",
      "f1_score: 0.13392638687086697"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy_score: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import  metrics\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('SVM linear', grid_1.best_estimator_))\n",
    "models.append(('NB', grid_2.best_estimator_))\n",
    "models.append(('SVM EBF', grid_3.best_estimator_))\n",
    "\n",
    "\n",
    "precision_score = []\n",
    "recall_score = []\n",
    "f1_score = []\n",
    "accuracy_score = []\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    print(\"precision_score: {}\".format(metrics.precision_score(y_test, model.predict(X_test), average='weighted') ))\n",
    "    print(\"recall_score: {}\".format( metrics.recall_score(y_test, model.predict(X_test), average='weighted') ))\n",
    "    print(\"f1_score: {}\".format( metrics.f1_score(y_test, model.predict(X_test), average='weighted') ))\n",
    "    print(\"accuracy_score: {}\".format( metrics.accuracy_score(y_test, model.predict(X_test)) ))\n",
    "    precision_score.append(metrics.precision_score(y_test, model.predict(X_test), average='weighted'))\n",
    "    recall_score.append(metrics.recall_score(y_test, model.predict(X_test), average='weighted'))\n",
    "    f1_score.append( metrics.f1_score(y_test, model.predict(X_test), average='weighted'))\n",
    "    accuracy_score.append(metrics.accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM linear</td>\n",
       "      <td>0.164574</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.152456</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.131135</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.140393</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM EBF</td>\n",
       "      <td>0.150798</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.133926</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Method  precision_score  recall_score  f1_score  accuracy_score\n",
       "0  SVM linear         0.164574         0.180  0.152456           0.180\n",
       "1          NB         0.131135         0.175  0.140393           0.175\n",
       "2     SVM EBF         0.150798         0.180  0.133926           0.180"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {'precision_score': precision_score, \n",
    "     'recall_score': recall_score, \n",
    "     'f1_score': f1_score,\n",
    "     'accuracy_score' : accuracy_score\n",
    "    }\n",
    "df = pd.DataFrame(data=d)\n",
    "df.insert(loc=0, column='Method', value=['SVM linear',\"NB\",\"SVM RBF\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
